{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wide_resnet.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S44TaN5ywbm8",
        "colab_type": "code",
        "outputId": "94bf3027-af89-4304-8d6d-f720091c1c81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# %mkdir /content/drive/'My Drive'/micronet\n",
        "%cd  /content/drive/'My Drive'/micronet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/micronet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkwWK0HKZyhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
        "\n",
        "def conv_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.xavier_uniform(m.weight, gain=np.sqrt(2))\n",
        "        init.constant_(m.bias, 0)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        init.constant_(m.weight, 1)\n",
        "        init.constant_(m.bias, 0)\n",
        "\n",
        "class wide_basic(nn.Module):\n",
        "    def __init__(self, in_planes, planes, dropout_rate, stride=1):\n",
        "        super(wide_basic, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, bias=True)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=True),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.dropout(self.conv1(F.relu(self.bn1(x))))\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out += self.shortcut(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Wide_ResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, dropout_rate, num_classes):\n",
        "        super(Wide_ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        assert ((depth-4)%6 ==0), 'Wide-resnet depth should be 6n+4'\n",
        "        n = (depth-4)/6\n",
        "        k = widen_factor\n",
        "\n",
        "        print('| Wide-Resnet %dx%d' %(depth, k))\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "\n",
        "        self.conv1 = conv3x3(3,nStages[0])\n",
        "        self.layer1 = self._wide_layer(wide_basic, nStages[1], n, dropout_rate, stride=1)\n",
        "        self.layer2 = self._wide_layer(wide_basic, nStages[2], n, dropout_rate, stride=2)\n",
        "        self.layer3 = self._wide_layer(wide_basic, nStages[3], n, dropout_rate, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(nStages[3], momentum=0.9)\n",
        "        self.linear = nn.Linear(nStages[3], num_classes)\n",
        "\n",
        "    def _wide_layer(self, block, planes, num_blocks, dropout_rate, stride):\n",
        "        strides = [stride] + [1]*(int(num_blocks)-1)\n",
        "        layers = []\n",
        "\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, dropout_rate, stride))\n",
        "            self.in_planes = planes\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.relu(self.bn1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     net=Wide_ResNet(28, 10, 0.3, 10)\n",
        "#     y = net(Variable(torch.randn(1,3,32,32)))\n",
        "\n",
        "#     print(y.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyzL-ZnyaTkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############### Pytorch CIFAR configuration file ###############\n",
        "import math\n",
        "\n",
        "start_epoch = 1\n",
        "num_epochs = 200\n",
        "batch_size = 128\n",
        "optim_type = 'SGD'\n",
        "\n",
        "mean = {\n",
        "    'cifar10': (0.4914, 0.4822, 0.4465),\n",
        "    'cifar100': (0.5071, 0.4867, 0.4408),\n",
        "}\n",
        "\n",
        "std = {\n",
        "    'cifar10': (0.2023, 0.1994, 0.2010),\n",
        "    'cifar100': (0.2675, 0.2565, 0.2761),\n",
        "}\n",
        "\n",
        "# Only for cifar-10\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "def learning_rate(init, epoch):\n",
        "    optim_factor = 0\n",
        "    if(epoch > 89):\n",
        "        optim_factor = 3\n",
        "    elif(epoch > 76): # prev it was 120\n",
        "        optim_factor = 2\n",
        "    elif(epoch > 60):\n",
        "        optim_factor = 1\n",
        "\n",
        "    return init*math.pow(0.2, optim_factor)\n",
        "\n",
        "def get_hms(seconds):\n",
        "    m, s = divmod(seconds, 60)\n",
        "    h, m = divmod(m, 60)\n",
        "\n",
        "    return h, m, s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S05UVHMZ5rh",
        "colab_type": "code",
        "outputId": "eec8ffe1-3f32-4cc9-e262-e4eaad35f476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "  \n",
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import argparse\n",
        "import datetime\n",
        "\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "lr = 0.1\n",
        "net_type = \"wide-resnet\"\n",
        "depth = 28\n",
        "widen_factor = 10\n",
        "dropout = 0.3\n",
        "dataset = \"cifar100\"\n",
        "resume = True \n",
        "testOnly = False\n",
        "\n",
        "# Hyper Parameter settings\n",
        "use_cuda = torch.cuda.is_available()\n",
        "best_acc = 0\n",
        "\n",
        "# Data Uplaod\n",
        "print('\\n[Phase 1] : Data Preparation')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean[dataset], std[dataset]),\n",
        "]) # meanstd transformation\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean[dataset], std[dataset]),\n",
        "])\n",
        "\n",
        "\n",
        "print(\"| Preparing CIFAR-100 dataset...\")\n",
        "sys.stdout.write(\"| \")\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=False, transform=transform_test)\n",
        "num_classes = 100\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "# Return network & file name\n",
        "def getNetwork():\n",
        "    net = Wide_ResNet(depth, widen_factor, dropout, num_classes)\n",
        "    file_name = 'wide-resnet-'+str(depth)+'x'+str(widen_factor)\n",
        "    return net, file_name\n",
        "\n",
        "# Test only option\n",
        "def test():\n",
        "    print('\\n[Test Phase] : Model setup')\n",
        "    assert os.path.isdir('checkpoint'), 'Error: No checkpoint directory found!'\n",
        "    _, file_name = getNetwork()\n",
        "    checkpoint = torch.load('./checkpoint/'+dataset+os.sep+file_name+'.t7')\n",
        "    net = checkpoint['net']\n",
        "\n",
        "    if use_cuda:\n",
        "        net.cuda()\n",
        "        net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "          if use_cuda:\n",
        "              inputs, targets = inputs.cuda(), targets.cuda()\n",
        "          outputs = net(inputs)\n",
        "\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += targets.size(0)\n",
        "          correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "      acc = 100.*correct/total\n",
        "      print(\"| Test Result\\tAcc@1: %.2f%%\" %(acc))\n",
        "\n",
        "\n",
        "# Model\n",
        "print('\\n[Phase 2] : Model setup')\n",
        "if resume:\n",
        "    # Load checkpoint\n",
        "    print('| Resuming from checkpoint...')\n",
        "    assert os.path.isdir('checkpoint'), 'Error: No checkpoint directory found!'\n",
        "    _, file_name = getNetwork()\n",
        "    checkpoint = torch.load('./checkpoint/'+dataset+os.sep+file_name+'.t7')\n",
        "    net = checkpoint['net']\n",
        "    best_acc = checkpoint['acc']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "else:\n",
        "    print('| Building net type [' + net_type + ']...')\n",
        "    net, file_name = getNetwork()\n",
        "    net.apply(conv_init)\n",
        "\n",
        "if use_cuda:\n",
        "    net.cuda()\n",
        "    net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training\n",
        "def train(epoch):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    optimizer = optim.SGD(net.parameters(), lr=learning_rate(lr, epoch), momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    print('\\n=> Training Epoch #%d, LR=%.4f' %(epoch, learning_rate(lr, epoch)))\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        if use_cuda:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda() # GPU settings\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)               # Forward Propagation\n",
        "        loss = criterion(outputs, targets)  # Loss\n",
        "        loss.backward()  # Backward Propagation\n",
        "        optimizer.step() # Optimizer update\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "        sys.stdout.write('\\r')\n",
        "        sys.stdout.write('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
        "                %(epoch, num_epochs, batch_idx+1,\n",
        "                    (len(trainset)//batch_size)+1, loss.item(), 100.*correct/total))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "          if use_cuda:\n",
        "              inputs, targets = inputs.cuda(), targets.cuda()\n",
        "          outputs = net(inputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "\n",
        "          test_loss += loss.item()\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += targets.size(0)\n",
        "          correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "      # Save checkpoint when best model\n",
        "      acc = 100.*correct/total\n",
        "      print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %(epoch, loss.item(), acc))\n",
        "\n",
        "      # if acc > best_acc:\n",
        "      print('| Saving Best model...\\t\\t\\tTop1 = %.2f%%' %(acc))\n",
        "      state = {\n",
        "              'net':net.module if use_cuda else net,\n",
        "              'acc':acc,\n",
        "              'epoch':epoch,\n",
        "      }\n",
        "      if not os.path.isdir('checkpoint'):\n",
        "          os.mkdir('checkpoint')\n",
        "      save_point = './checkpoint/'+dataset+os.sep\n",
        "      if not os.path.isdir(save_point):\n",
        "          os.mkdir(save_point)\n",
        "      torch.save(state, save_point+file_name+'.t7')\n",
        "      best_acc = acc\n",
        "\n",
        "print('\\n[Phase 3] : Training model')\n",
        "print('| Training Epochs = ' + str(num_epochs))\n",
        "print('| Initial Learning Rate = ' + str(lr))\n",
        "print('| Optimizer = ' + str(optim_type))\n",
        "\n",
        "elapsed_time = 0\n",
        "for epoch in range(start_epoch, start_epoch+num_epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "    elapsed_time += epoch_time\n",
        "    print('| Elapsed time : %d:%02d:%02d'  %(get_hms(elapsed_time)))\n",
        "\n",
        "print('\\n[Phase 4] : Testing model')\n",
        "print('* Test results : Acc@1 = %.2f%%' %(best_acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Phase 1] : Data Preparation\n",
            "| Preparing CIFAR-100 dataset...\n",
            "| Files already downloaded and verified\n",
            "\n",
            "[Phase 2] : Model setup\n",
            "| Resuming from checkpoint...\n",
            "| Wide-Resnet 28x10\n",
            "\n",
            "[Phase 3] : Training model\n",
            "| Training Epochs = 200\n",
            "| Initial Learning Rate = 0.1\n",
            "| Optimizer = SGD\n",
            "\n",
            "=> Training Epoch #88, LR=0.0040\n",
            "| Epoch [ 88/200] Iter[391/391]\t\tLoss: 0.0131 Acc@1: 99.000%\n",
            "| Validation Epoch #88\t\t\tLoss: 0.8938 Acc@1: 78.00%\n",
            "| Saving Best model...\t\t\tTop1 = 78.00%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Wide_ResNet. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type wide_basic. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| Elapsed time : 0:04:56\n",
            "\n",
            "=> Training Epoch #89, LR=0.0040\n",
            "| Epoch [ 89/200] Iter[391/391]\t\tLoss: 0.0370 Acc@1: 99.000%\n",
            "| Validation Epoch #89\t\t\tLoss: 0.9738 Acc@1: 78.00%\n",
            "| Saving Best model...\t\t\tTop1 = 78.00%\n",
            "| Elapsed time : 0:09:52\n",
            "\n",
            "=> Training Epoch #90, LR=0.0008\n",
            "| Epoch [ 90/200] Iter[391/391]\t\tLoss: 0.0169 Acc@1: 99.000%\n",
            "| Validation Epoch #90\t\t\tLoss: 1.0305 Acc@1: 78.00%\n",
            "| Saving Best model...\t\t\tTop1 = 78.00%\n",
            "| Elapsed time : 0:14:48\n",
            "\n",
            "=> Training Epoch #91, LR=0.0008\n",
            "| Epoch [ 91/200] Iter[391/391]\t\tLoss: 0.0106 Acc@1: 99.000%\n",
            "| Validation Epoch #91\t\t\tLoss: 0.9206 Acc@1: 79.00%\n",
            "| Saving Best model...\t\t\tTop1 = 79.00%\n",
            "| Elapsed time : 0:19:44\n",
            "\n",
            "=> Training Epoch #92, LR=0.0008\n",
            "| Epoch [ 92/200] Iter[391/391]\t\tLoss: 0.0160 Acc@1: 99.000%\n",
            "| Validation Epoch #92\t\t\tLoss: 0.9434 Acc@1: 79.00%\n",
            "| Saving Best model...\t\t\tTop1 = 79.00%\n",
            "| Elapsed time : 0:24:41\n",
            "\n",
            "=> Training Epoch #93, LR=0.0008\n",
            "| Epoch [ 93/200] Iter[391/391]\t\tLoss: 0.0110 Acc@1: 99.000%\n",
            "| Validation Epoch #93\t\t\tLoss: 0.9831 Acc@1: 78.00%\n",
            "| Saving Best model...\t\t\tTop1 = 78.00%\n",
            "| Elapsed time : 0:29:37\n",
            "\n",
            "=> Training Epoch #94, LR=0.0008\n",
            "| Epoch [ 94/200] Iter[391/391]\t\tLoss: 0.0261 Acc@1: 99.000%\n",
            "| Validation Epoch #94\t\t\tLoss: 0.8986 Acc@1: 79.00%\n",
            "| Saving Best model...\t\t\tTop1 = 79.00%\n",
            "| Elapsed time : 0:34:33\n",
            "\n",
            "=> Training Epoch #95, LR=0.0008\n",
            "| Epoch [ 95/200] Iter[391/391]\t\tLoss: 0.0128 Acc@1: 99.000%\n",
            "| Validation Epoch #95\t\t\tLoss: 0.9649 Acc@1: 78.00%\n",
            "| Saving Best model...\t\t\tTop1 = 78.00%\n",
            "| Elapsed time : 0:39:29\n",
            "\n",
            "=> Training Epoch #96, LR=0.0008\n",
            "| Epoch [ 96/200] Iter[369/391]\t\tLoss: 0.0104 Acc@1: 99.000%"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}